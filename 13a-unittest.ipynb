{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python testing support in the `unittest` library\n",
    "\n",
    "Normal method of use:\n",
    "\n",
    "- Create a subclass of `unittest.TestCase`\n",
    "- Write a bunch of test methods\n",
    "- (optional) Put the following code at the bottom of your test file:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "\n",
    "You can also run all the tests in a file by invoking the `unittest` module directly:\n",
    "\n",
    "```bash\n",
    "$ python -m unittest mytestfile.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test1.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test1.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    def test_pass(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run in 'verbose' mode to see which tests were run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pass (data.test-examples.test1.MyTest) ... ok\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test1.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: python -m unittest [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\r\n",
      "                          [-k TESTNAMEPATTERNS]\r\n",
      "                          [tests [tests ...]]\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  tests                a list of any number of test modules, classes and test\r\n",
      "                       methods.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help           show this help message and exit\r\n",
      "  -v, --verbose        Verbose output\r\n",
      "  -q, --quiet          Quiet output\r\n",
      "  --locals             Show local variables in tracebacks\r\n",
      "  -f, --failfast       Stop on first fail or error\r\n",
      "  -c, --catch          Catch Ctrl-C and display results so far\r\n",
      "  -b, --buffer         Buffer stdout and stderr during tests\r\n",
      "  -k TESTNAMEPATTERNS  Only run tests which match the given substring\r\n",
      "\r\n",
      "Examples:\r\n",
      "  python -m unittest test_module               - run tests from test_module\r\n",
      "  python -m unittest module.TestClass          - run tests from module.TestClass\r\n",
      "  python -m unittest module.Class.test_method  - run specified test method\r\n",
      "  python -m unittest path/to/test_file.py      - run tests from test_file.py\r\n",
      "\r\n",
      "usage: python -m unittest discover [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\r\n",
      "                                   [-k TESTNAMEPATTERNS] [-s START]\r\n",
      "                                   [-p PATTERN] [-t TOP]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -v, --verbose         Verbose output\r\n",
      "  -q, --quiet           Quiet output\r\n",
      "  --locals              Show local variables in tracebacks\r\n",
      "  -f, --failfast        Stop on first fail or error\r\n",
      "  -c, --catch           Catch Ctrl-C and display results so far\r\n",
      "  -b, --buffer          Buffer stdout and stderr during tests\r\n",
      "  -k TESTNAMEPATTERNS   Only run tests which match the given substring\r\n",
      "  -s START, --start-directory START\r\n",
      "                        Directory to start discovery ('.' default)\r\n",
      "  -p PATTERN, --pattern PATTERN\r\n",
      "                        Pattern to match tests ('test*.py' default)\r\n",
      "  -t TOP, --top-level-directory TOP\r\n",
      "                        Top level directory of project (defaults to start\r\n",
      "                        directory)\r\n",
      "\r\n",
      "For test discovery all test modules must be importable from the top level\r\n",
      "directory of the project.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failures and Errors\n",
    "\n",
    "In `unittest`, an `AssertionError` is considered a test \"failure\". Any other exception raised by the test that is not handled is considered a test \"error\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test2.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test2.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_fail(self):\n",
    "        x = 'This is local'\n",
    "        y = 1+1\n",
    "        assert y == 2\n",
    "        assert False\n",
    "#         if not False:\n",
    "#             raise AssertionError\n",
    "\n",
    "    def test_fail_message(self):\n",
    "        assert False, 'This is an assértion message'\n",
    "#         if not False:\n",
    "#             raise AssertionError('This is an assértion message')\n",
    "\n",
    "    def test_math(self):\n",
    "        assert 1 + 1 == 2, 'Math is broken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF.\r\n",
      "======================================================================\r\n",
      "FAIL: test_fail (data.test-examples.test2.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test2.py\", line 9, in test_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_fail_message (data.test-examples.test2.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test2.py\", line 14, in test_fail_message\r\n",
      "    assert False, 'This is an assértion message'\r\n",
      "AssertionError: This is an assértion message\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=2)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_fail (data.test-examples.test2.MyTest) ... FAIL\r\n",
      "test_fail_message (data.test-examples.test2.MyTest) ... FAIL\r\n",
      "test_math (data.test-examples.test2.MyTest) ... ok\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_fail (data.test-examples.test2.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test2.py\", line 9, in test_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_fail_message (data.test-examples.test2.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test2.py\", line 14, in test_fail_message\r\n",
      "    assert False, 'This is an assértion message'\r\n",
      "AssertionError: This is an assértion message\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=2)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using assertion helpers\n",
    "\n",
    "While we can use bare `assert` statements or manually raise `AssertionError`, it's usually better to use `unittest.TestCase`'s helper methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/simple_math.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/simple_math.py\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "def divide(a, b):\n",
    "    return a / b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test3.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test3.py\n",
    "import unittest\n",
    "from . import simple_math\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_and_one(self):\n",
    "        self.assertEqual(simple_math.add(1, 1), 2)\n",
    "\n",
    "    def test_one_and_one_fail(self):\n",
    "        self.assertEqual(simple_math.add(1, 1), 4)\n",
    "\n",
    "    def test_one_and_one_fail_assert(self):\n",
    "        assert simple_math.add(1,1) == 4\n",
    "\n",
    "    def test_some_lists(self):\n",
    "        self.assertEqual([1, 2, 4], [1, 2, 3, 4])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_one_and_one (data.test-examples.test3.MyTest) ... ok\r\n",
      "test_one_and_one_fail (data.test-examples.test3.MyTest) ... FAIL\r\n",
      "test_one_and_one_fail_assert (data.test-examples.test3.MyTest) ... FAIL\r\n",
      "test_some_lists (data.test-examples.test3.MyTest) ... FAIL\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_one_and_one_fail (data.test-examples.test3.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test3.py\", line 10, in test_one_and_one_fail\r\n",
      "    self.assertEqual(simple_math.add(1, 1), 4)\r\n",
      "AssertionError: 2 != 4\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_one_and_one_fail_assert (data.test-examples.test3.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test3.py\", line 13, in test_one_and_one_fail_assert\r\n",
      "    assert simple_math.add(1,1) == 4\r\n",
      "AssertionError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_some_lists (data.test-examples.test3.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test3.py\", line 16, in test_some_lists\r\n",
      "    self.assertEqual([1, 2, 4], [1, 2, 3, 4])\r\n",
      "AssertionError: Lists differ: [1, 2, 4] != [1, 2, 3, 4]\r\n",
      "\r\n",
      "First differing element 2:\r\n",
      "4\r\n",
      "3\r\n",
      "\r\n",
      "Second list contains 1 additional elements.\r\n",
      "First extra element 3:\r\n",
      "4\r\n",
      "\r\n",
      "- [1, 2, 4]\r\n",
      "+ [1, 2, 3, 4]\r\n",
      "?        +++\r\n",
      "\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=3)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test3.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3 == 0.1 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000000000000004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.assertAlmostEqual(0.3, 0.1 * 3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "0.3 != 0.30000000000000004",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6614e7087644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/unittest/case.py\u001b[0m in \u001b[0;36massertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \"\"\"\n\u001b[1;32m    911\u001b[0m         \u001b[0massertion_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getAssertEqualityFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         \u001b[0massertion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertNotEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/unittest/case.py\u001b[0m in \u001b[0;36m_baseAssertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mstandardMsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s != %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_common_shorten_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 0.3 != 0.30000000000000004"
     ]
    }
   ],
   "source": [
    "tc.assertEqual(0.3, 0.1*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP math is slightly broken\n"
     ]
    }
   ],
   "source": [
    "if 0.3 == 0.1 * 3:\n",
    "    print('This does not happen')\n",
    "else:\n",
    "    print('FP math is slightly broken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.551115123125783e-17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3 - (0.1 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Exceptions\n",
    "\n",
    "You can ensure that expected exceptions are raised manually, or by using the `assertRaises` helpers in `TestCase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test4.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test4.py\n",
    "import unittest\n",
    "from . import simple_math\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_and_one_alt(self):\n",
    "        try:\n",
    "            simple_math.divide(1,0)\n",
    "        except ZeroDivisionError:\n",
    "            return # pass\n",
    "        else:\n",
    "            raise AssertionError('ZeroDivisionError was not raised')\n",
    "\n",
    "    def test_one_and_one(self):\n",
    "        self.assertRaises(\n",
    "            ZeroDivisionError, simple_math.divide, 1, 0)\n",
    "\n",
    "    def test_one_and_one_alt2(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            simple_math.divide(1, 0)\n",
    "\n",
    "    def test_one_and_one_alt3(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            1 / 0\n",
    "\n",
    "    def test_one_and_one_alt3_fail(self):'\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            1 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 5 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test errors\n",
    "\n",
    "Test \"errors\" are displayed differently from test \"failures.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test5.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test5.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_pass(self):\n",
    "        pass\n",
    "\n",
    "    def test_fail(self):\n",
    "        assert False\n",
    "\n",
    "    def test_also_fail(self):\n",
    "        raise AssertionError()\n",
    "\n",
    "    def test_error(self):\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_also_fail (data.test-examples.test5.MyTest) ... FAIL\r\n",
      "test_error (data.test-examples.test5.MyTest) ... ERROR\r\n",
      "test_fail (data.test-examples.test5.MyTest) ... FAIL\r\n",
      "test_pass (data.test-examples.test5.MyTest) ... ok\r\n",
      "\r\n",
      "======================================================================\r\n",
      "ERROR: test_error (data.test-examples.test5.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test5.py\", line 15, in test_error\r\n",
      "    raise ValueError()\r\n",
      "ValueError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_also_fail (data.test-examples.test5.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test5.py\", line 12, in test_also_fail\r\n",
      "    raise AssertionError()\r\n",
      "AssertionError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_fail (data.test-examples.test5.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test5.py\", line 9, in test_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=2, errors=1)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test5.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup / teardown code\n",
    "\n",
    "If you are always writing the same set of code at the beginning/end of your tests, you can refactor it into a `setUp` and/or `tearDown` method in your `TestCase`.\n",
    "\n",
    "Note that `setUp` and `tearDown` are called before/after *each* test, not once at the beginning of the suite and once at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test6.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test6.py\n",
    "import unittest\n",
    "from .simple_math import add, subtract, multiply, divide\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print('setUpClass')\n",
    "        # assert False  # will prevent test methods from running\n",
    "        \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print('tearDownClass')\n",
    "\n",
    "    def setUp(self):\n",
    "        self.x = 1\n",
    "        self.y = 1\n",
    "        print('setUp')\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('tearDown')\n",
    "\n",
    "    def test_add(self):\n",
    "        print('test_add')\n",
    "        self.assertEqual(add(self.x, self.y), 2)\n",
    "\n",
    "    def test_subtract(self):\n",
    "        self.assertEqual(subtract(self.x, self.y), 0)\n",
    "\n",
    "    def test_multiply(self):\n",
    "        self.assertEqual(multiply(self.x, self.y), 1)\n",
    "\n",
    "    def test_divide(self):\n",
    "        self.assertEqual(divide(self.x, self.y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUpClass\r\n",
      "test_add (data.test-examples.test6.MyTest) ... setUp\r\n",
      "test_add\r\n",
      "tearDown\r\n",
      "ok\r\n",
      "test_divide (data.test-examples.test6.MyTest) ... setUp\r\n",
      "tearDown\r\n",
      "ok\r\n",
      "test_multiply (data.test-examples.test6.MyTest) ... setUp\r\n",
      "tearDown\r\n",
      "ok\r\n",
      "test_subtract (data.test-examples.test6.MyTest) ... setUp\r\n",
      "tearDown\r\n",
      "ok\r\n",
      "tearDownClass\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test6.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docstrings in tests\n",
    "\n",
    "If your test name is not enough to identify exactly what's being tested, you can add a docstring to your test methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test7.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test7.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \"TestCase docstring\"\n",
    "\n",
    "    def test_docstring(self):\n",
    "        \"This is a test docstring. It should say what's being tested.\"\n",
    "        pass\n",
    "\n",
    "    def test_no_docstring(self):\n",
    "        pass\n",
    "\n",
    "    def test_docstring_fail(self):\n",
    "        \"\"\"This is a test docstring. It should say what's being tested.\n",
    "        \n",
    "        Multi-line is omitted from the test output.\n",
    "        \"\"\"\n",
    "        assert False\n",
    "\n",
    "    def test_no_docstring_fail(self):\n",
    "        assert False\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".F.F\r\n",
      "======================================================================\r\n",
      "FAIL: test_docstring_fail (data.test-examples.test7.MyTest)\r\n",
      "This is a test docstring. It should say what's being tested.\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test7.py\", line 18, in test_docstring_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_no_docstring_fail (data.test-examples.test7.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test7.py\", line 21, in test_no_docstring_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=2)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_docstring (data.test-examples.test7.MyTest)\r\n",
      "This is a test docstring. It should say what's being tested. ... ok\r\n",
      "test_docstring_fail (data.test-examples.test7.MyTest)\r\n",
      "This is a test docstring. It should say what's being tested. ... FAIL\r\n",
      "test_no_docstring (data.test-examples.test7.MyTest) ... ok\r\n",
      "test_no_docstring_fail (data.test-examples.test7.MyTest) ... FAIL\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_docstring_fail (data.test-examples.test7.MyTest)\r\n",
      "This is a test docstring. It should say what's being tested.\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test7.py\", line 18, in test_docstring_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_no_docstring_fail (data.test-examples.test7.MyTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/rick446/src/arborian-classes/src/data/test-examples/test7.py\", line 21, in test_no_docstring_fail\r\n",
      "    assert False\r\n",
      "AssertionError\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=2)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest data/test-examples/test7.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doctest: testing your documentation\n",
    "\n",
    "If you include little snippets of interpreter sessions in your application's docstrings, you can test to ensure that the documentation actually runs correctly by invoking the `doctest` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/test9.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/test9.py\n",
    "def concat(values):\n",
    "    '''Concatenate multiple strings\n",
    "\n",
    "    >>> concat(['foo', 'bar', 'baz'])\n",
    "    'foobarbaz'\n",
    "    >>> concat(['foo', ' bar ', 'baz'])\n",
    "    'foo bar baz'\n",
    "    >>> concat(['foo', ' bar ', 5])\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    TypeError: can only concatenate str (not \"int\") to str\n",
    "    '''\n",
    "    result = ''\n",
    "    for value in values:\n",
    "        result += value\n",
    "    return result\n",
    "\n",
    "\n",
    "def average(values):\n",
    "    \"\"\"Computes the arithmetic mean of a list of numbers.\n",
    "\n",
    "    >>> average([20, 30, 70])\n",
    "    40.0\n",
    "    \"\"\"\n",
    "    return sum(values, 0.0) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest data/test-examples/test9.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\r\n",
      "    average([20, 30, 70])\r\n",
      "Expecting:\r\n",
      "    40.0\r\n",
      "ok\r\n",
      "Trying:\r\n",
      "    concat(['foo', 'bar', 'baz'])\r\n",
      "Expecting:\r\n",
      "    'foobarbaz'\r\n",
      "ok\r\n",
      "Trying:\r\n",
      "    concat(['foo', ' bar ', 'baz'])\r\n",
      "Expecting:\r\n",
      "    'foo bar baz'\r\n",
      "ok\r\n",
      "Trying:\r\n",
      "    concat(['foo', ' bar ', 5])\r\n",
      "Expecting:\r\n",
      "    Traceback (most recent call last):\r\n",
      "    ...\r\n",
      "    TypeError: can only concatenate str (not \"int\") to str\r\n",
      "ok\r\n",
      "1 items had no tests:\r\n",
      "    test9\r\n",
      "2 items passed all tests:\r\n",
      "   1 tests in test9.average\r\n",
      "   3 tests in test9.concat\r\n",
      "4 tests in 3 items.\r\n",
      "4 passed and 0 failed.\r\n",
      "Test passed.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m doctest data/test-examples/test9.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring test coverage\n",
    "\n",
    "The `coverage` third-party module provides summary information about what parts of your application your test code has exercised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /home/rick446/src/wheelhouse\n",
      "Requirement already satisfied: coverage in /home/rick446/.virtualenvs/classes/lib/python3.8/site-packages (5.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/rick446/.virtualenvs/classes/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage.py, version 5.3 with C extension\r\n",
      "Measure, collect, and report on code coverage in Python programs.\r\n",
      "\r\n",
      "usage: coverage <command> [options] [args]\r\n",
      "\r\n",
      "Commands:\r\n",
      "    annotate    Annotate source files with execution information.\r\n",
      "    combine     Combine a number of data files.\r\n",
      "    debug       Display information about the internals of coverage.py\r\n",
      "    erase       Erase previously collected coverage data.\r\n",
      "    help        Get help on using coverage.py.\r\n",
      "    html        Create an HTML report.\r\n",
      "    json        Create a JSON report of coverage results.\r\n",
      "    report      Report coverage stats on modules.\r\n",
      "    run         Run a Python program and measure code execution.\r\n",
      "    xml         Create an XML report of coverage results.\r\n",
      "\r\n",
      "Use \"coverage help <command>\" for detailed help on any command.\r\n",
      "Full documentation is at https://coverage.readthedocs.io\r\n"
     ]
    }
   ],
   "source": [
    "!python -m coverage help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage.py, version 5.3 with C extension\r\n",
      "Measure, collect, and report on code coverage in Python programs.\r\n",
      "\r\n",
      "usage: coverage <command> [options] [args]\r\n",
      "\r\n",
      "Commands:\r\n",
      "    annotate    Annotate source files with execution information.\r\n",
      "    combine     Combine a number of data files.\r\n",
      "    debug       Display information about the internals of coverage.py\r\n",
      "    erase       Erase previously collected coverage data.\r\n",
      "    help        Get help on using coverage.py.\r\n",
      "    html        Create an HTML report.\r\n",
      "    json        Create a JSON report of coverage results.\r\n",
      "    report      Report coverage stats on modules.\r\n",
      "    run         Run a Python program and measure code execution.\r\n",
      "    xml         Create an XML report of coverage results.\r\n",
      "\r\n",
      "Use \"coverage help <command>\" for detailed help on any command.\r\n",
      "Full documentation is at https://coverage.readthedocs.io\r\n"
     ]
    }
   ],
   "source": [
    "!coverage help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/cards.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/cards.py\n",
    "ranks = '2 3 4 5 6 7 8 9 10 J Q K A'.split()\n",
    "suits = 'spades hearts clubs diamonds'.split()\n",
    "\n",
    "class Card:    \n",
    "    def __init__(self, rank, suit):\n",
    "        if rank not in ranks:\n",
    "            raise ValueError('invalid rank')  # pragma: no cover\n",
    "        if suit not in suits:\n",
    "            raise ValueError('invalid suit')\n",
    "        self.rank, self.suit = rank, suit\n",
    "        \n",
    "    def __eq__(self, other):   \n",
    "        return self.rank == other.rank and self.suit == other.suit\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.rank, self.suit))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.rank} {self.suit}'\n",
    "    \n",
    "    \n",
    "class CardStack:\n",
    "    \n",
    "    def __init__(self, cards):\n",
    "        self.cards = list(cards)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cards)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.cards[i]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ' '.join(repr(c) for c in self)\n",
    "    \n",
    "    \n",
    "class Deck(CardStack):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(Card(r, s) for r in ranks for s in suits)\n",
    "\n",
    "    def __setitem__(self, i, value):\n",
    "        self.cards[i] = value\n",
    "\n",
    "    def deal(self, n):\n",
    "        return Hand([self.cards.pop() for i in range(n)])\n",
    "    \n",
    "    def draw(self, hand):\n",
    "        hand.add(self.cards.pop())\n",
    "    \n",
    "\n",
    "class Hand(CardStack):\n",
    "    \n",
    "    def score(self):\n",
    "        aces = [c for c in self if c.rank == 'A']\n",
    "        others = [c for c in self if c.rank != 'A']\n",
    "        subtotal = sum(\n",
    "            int(c.rank) if c.rank.isdigit() else 10\n",
    "            for c in others)\n",
    "        subtotal += 11 * len(aces)\n",
    "        while subtotal > 21 and aces:\n",
    "            aces.pop()\n",
    "            subtotal -= 10\n",
    "        return subtotal\n",
    "    \n",
    "    def add(self, card):\n",
    "        self.cards.append(card)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test-examples/card-test.py\n"
     ]
    }
   ],
   "source": [
    "%%file data/test-examples/card-test.py\n",
    "import unittest\n",
    "\n",
    "from cards import Hand, Card\n",
    "\n",
    "class TestHand(unittest.TestCase):\n",
    "    \n",
    "    def test_simple(self):\n",
    "        hand = Hand([Card(rank='5', suit='spades')])\n",
    "        self.assertEqual(hand.score(), 5)\n",
    "        \n",
    "    def test_soft_17(self):\n",
    "        hand = Hand([\n",
    "            Card(rank='A', suit='spades'),\n",
    "            Card(rank='6', suit='spades'),\n",
    "        ])\n",
    "        self.assertEqual(hand.score(), 17)\n",
    "            \n",
    "    def test_hard_17(self):\n",
    "        hand = Hand([\n",
    "            Card(rank='A', suit='spades'),\n",
    "            Card(rank='K', suit='spades'),\n",
    "            Card(rank='6', suit='spades'),\n",
    "        ])\n",
    "        self.assertEqual(hand.score(), 17)\n",
    "        \n",
    "    def test_really_hard_14(self):\n",
    "        hand = Hand([\n",
    "            Card(rank='A', suit='spades'),\n",
    "            Card(rank='A', suit='clubs'),\n",
    "            Card(rank='A', suit='hearts'),\n",
    "            Card(rank='A', suit='diamonds'),\n",
    "            Card(rank='K', suit='spades')\n",
    "        ])\n",
    "        self.assertEqual(hand.score(), 14)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                                                                            Stmts   Miss  Cover   Missing\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "/home/rick446/.virtualenvs/classes/lib/python3.8/site-packages/_virtualenv.py      81     80     1%   4-54, 57-130\n",
      "card-test.py                                                                       15      0   100%\n",
      "cards.py                                                                           44     11    75%   9, 13, 16, 19, 28, 34, 40, 43, 46, 49, 67\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "TOTAL                                                                             140     91    35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd data/test-examples\n",
    "coverage run -m unittest card-test.py\n",
    "coverage report -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ranks = '2 3 4 5 6 7 8 9 10 J Q K A'.split()\n",
      "> suits = 'spades hearts clubs diamonds'.split()\n",
      "  \n",
      "> class Card:    \n",
      ">     def __init__(self, rank, suit):\n",
      ">         if rank not in ranks:\n",
      "-             raise ValueError('invalid rank')  # pragma: no cover\n",
      ">         if suit not in suits:\n",
      "!             raise ValueError('invalid suit')\n",
      ">         self.rank, self.suit = rank, suit\n",
      "          \n",
      ">     def __eq__(self, other):   \n",
      "!         return self.rank == other.rank and self.suit == other.suit\n",
      "      \n",
      ">     def __hash__(self):\n",
      "!         return hash((self.rank, self.suit))\n",
      "      \n",
      ">     def __repr__(self):\n",
      "!         return f'{self.rank} {self.suit}'\n",
      "      \n",
      "      \n",
      "> class CardStack:\n",
      "      \n",
      ">     def __init__(self, cards):\n",
      ">         self.cards = list(cards)\n",
      "          \n",
      ">     def __len__(self):\n",
      "!         return len(self.cards)\n",
      "      \n",
      ">     def __getitem__(self, i):\n",
      ">         return self.cards[i]\n",
      "      \n",
      ">     def __repr__(self):\n",
      "!         return ' '.join(repr(c) for c in self)\n",
      "      \n",
      "      \n",
      "> class Deck(CardStack):\n",
      "      \n",
      ">     def __init__(self):\n",
      "!         super().__init__(Card(r, s) for r in ranks for s in suits)\n",
      "  \n",
      ">     def __setitem__(self, i, value):\n",
      "!         self.cards[i] = value\n",
      "  \n",
      ">     def deal(self, n):\n",
      "!         return Hand([self.cards.pop() for i in range(n)])\n",
      "      \n",
      ">     def draw(self, hand):\n",
      "!         hand.add(self.cards.pop())\n",
      "      \n",
      "  \n",
      "> class Hand(CardStack):\n",
      "      \n",
      ">     def score(self):\n",
      ">         aces = [c for c in self if c.rank == 'A']\n",
      ">         others = [c for c in self if c.rank != 'A']\n",
      ">         subtotal = sum(\n",
      ">             int(c.rank) if c.rank.isdigit() else 10\n",
      ">             for c in others)\n",
      ">         subtotal += 11 * len(aces)\n",
      ">         while subtotal > 21 and aces:\n",
      ">             aces.pop()\n",
      ">             subtotal -= 10\n",
      ">         return subtotal\n",
      "      \n",
      ">     def add(self, card):\n",
      "!         self.cards.append(card)\n",
      "              \n",
      "      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd data/test-examples\n",
    "coverage run -m unittest card-test.py\n",
    "coverage annotate\n",
    "cat ./cards.py,cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://coverage.readthedocs.io/en/6.3.2/config.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cond:\n",
    "    do_thing()\n",
    "do_other_thing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "Open [Testing lab][unittest-lab]\n",
    "\n",
    "[unittest-lab]: ./unittest-lab.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
